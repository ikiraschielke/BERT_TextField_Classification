{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#### implementation of the pytorch bert model\r\n",
        "#!pip install huggingface-hub\r\n",
        "#!pip install --upgrade tensorflow-gpu # for Python 3.n and GPU\r\n",
        "\r\n",
        "#https://www.kaggle.com/dhruvag/bert-multi-class-classification-for-large-text\r\n",
        "\r\n",
        "#https://www.philschmid.de/bert-text-classification-in-a-different-language\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\r\n",
        "\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "#model bert-base-german-dbmdz-cased https://github.com/dbmdz/berts\r\n",
        "\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "'''\r\n",
        "BEFORE RUNNING THIS NOTEBOOK MAKE SURE THO AHNGE 7ADJUST THE NAMES OF THE OUTPUT .CSV AND .PTH FILE FOR SAVING THE RESULTS AND TRAINED  MODEL\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1632991354269
        },
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "downsample = False\r\n",
        "\r\n",
        "if downsample:\r\n",
        "    # requires Dataframe to have two specific column names label and cat label, \r\n",
        "    # those are needed to assess the label distribution and will be dropped afterwards\r\n",
        "\r\n",
        "    df = pd.read_csv('to_one_hot.csv',index_col=0)\r\n",
        "    num_labels = len(df.iloc[:,1].unique())\r\n",
        "    print(num_labels)\r\n",
        "    y = pd.get_dummies(df.label, prefix='label')\r\n",
        "    #print(y.head())\r\n",
        "\r\n",
        "    ddf = pd.concat([df.iloc[:,0:3], y], axis=1)\r\n",
        "    print(ddf.columns, ddf.shape,ddf.head())\r\n",
        "\r\n",
        "    #print(ddf.columns)\r\n",
        "    col_order = ddf.columns\r\n",
        "    # nichts herunter sampeln!\r\n",
        "    target_count = ddf.label.value_counts()\r\n",
        "\r\n",
        "    print('Class 0:', target_count[0])\r\n",
        "    print('Class 1:', target_count[1:])\r\n",
        "    #print('Proportion:', round(target_count[0] / target_count[1:], 2), ': 1')\r\n",
        "\r\n",
        "    target_count.plot(kind='bar', title='Count (target)');\r\n",
        "\r\n",
        "    # https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\r\n",
        "    # EnthaÃ¶ltung und nichts auf 250 herunter samplen\r\n",
        "    # class 1 nichts hat 250, undersample class 0 und class 1\r\n",
        "    # Divide by class\r\n",
        "    if 'cat labels' in df.columns:\r\n",
        "\r\n",
        "        df_class_0 = ddf[ddf['cat labels'] == 0]\r\n",
        "        df_class_1 = ddf[ddf['cat labels'] == 1]\r\n",
        "        df_class_A = ddf[ddf['cat labels'] > 1]\r\n",
        "    else:\r\n",
        "        df_class_0 = ddf[ddf['label'] == 'Nichts']\r\n",
        "        df_class_1 = ddf[ddf['label'] == 'Enthaltung']\r\n",
        "        df_class_A = ddf[~ddf['label'].str.contains( 'Nichts| Sonstiges')]\r\n",
        "\r\n",
        "    df_class_0_under = df_class_0.sample(n=250)\r\n",
        "    df_class_1_under = df_class_1.sample(n=250)\r\n",
        "    df_sampled = pd.concat([df_class_0_under, df_class_1_under,df_class_A])\r\n",
        "    print(df_sampled.shape)\r\n",
        "\r\n",
        "    if 'cat labels' in df.columns:\r\n",
        "\r\n",
        "        print('Random under-sampling:')\r\n",
        "        print(df_sampled['cat labels'].value_counts())\r\n",
        "\r\n",
        "        df_sampled['cat labels'].value_counts().plot(kind='bar', title='Count (target)');\r\n",
        "\r\n",
        "        df = df_sampled.drop(['label', 'cat labels'], axis=1)\r\n",
        "\r\n",
        "    else:\r\n",
        "\r\n",
        "        print('Random under-sampling:')\r\n",
        "        df_sampled['label'].value_counts().plot(kind='bar', title='Count (target)')\r\n",
        "        df = df_sampled.drop(['label'], axis=1)\r\n",
        "\r\n",
        "else:\r\n",
        "    df = pd.read_csv('one_label_one_hot_nkm_open.csv',index_col=0)\r\n",
        "    num_labels = len(df.columns)-1\r\n",
        "    print(num_labels)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "answers = df.iloc[:,0].values\r\n",
        "print(answers[:5])\r\n",
        "\r\n",
        "#str_labels = df.iloc[:,1].to_list()\r\n",
        "labels = df.iloc[:,1:].values\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "# line 13 to overcome HTTPError 403 rate limit exceeded\r\n",
        "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n",
        "BertTokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-german-dbmdz-cased') \r\n",
        "\r\n",
        "\r\n",
        "# For every sentence...\r\n",
        "for sent in answers:\r\n",
        "    # `encode_plus` will:\r\n",
        "    #   (1) Tokenize the sentence.\r\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "    #   (3) Append the `[SEP]` token to the end.\r\n",
        "    #   (4) Map tokens to their IDs.\r\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "    #   (6) Create attention masks for [PAD] tokens.\r\n",
        "    encoded_dict = BertTokenizer.encode_plus(\r\n",
        "                        sent,                      # Sentence to encode.\r\n",
        "                        add_special_tokens=True,\r\n",
        "                        max_length=512,\r\n",
        "                        return_token_type_ids=False,\r\n",
        "                        padding=\"max_length\",\r\n",
        "                        truncation=True,\r\n",
        "                        return_attention_mask=True,\r\n",
        "                        return_tensors='pt',\r\n",
        "                   )\r\n",
        "    \r\n",
        "    # Add the encoded sentence to the list.    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    \r\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "int_labels = torch.FloatTensor(labels)\r\n",
        "\r\n",
        "\r\n",
        "# Print sentence 0, now as a list of IDs.\r\n",
        "print('Original: ', answers[0])\r\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['Nichts' 'Nichts' 'Nichts' 'Nichts' 'Nichts']\n[[0 0 1 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n ...\n [0 1 0 ... 0 0 0]\n [0 0 0 ... 0 0 1]\n [0 0 0 ... 0 0 0]]\n<class 'numpy.ndarray'>\n<class 'torch.Tensor'>\nOriginal:  Nichts\nToken IDs: tensor([ 102, 8009,  103,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0])\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using cache found in /home/azureuser/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632991358789
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\r\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "print(input_ids.shape, attention_masks.shape, int_labels.shape)\r\n",
        "\r\n",
        "# Combine the training inputs into a TensorDataset.\r\n",
        "dataset = TensorDataset(input_ids, attention_masks, int_labels)\r\n",
        "\r\n",
        "# Create a 90-10 train-validation split.\r\n",
        "\r\n",
        "# Calculate the number of samples to include in each set.\r\n",
        "train_size = int(0.9 * len(dataset))\r\n",
        "val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "# Divide the dataset by randomly selecting samples.\r\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
        "\r\n",
        "print('{:>5,} training samples'.format(train_size))\r\n",
        "print('{:>5,} validation samples'.format(val_size))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \r\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n",
        "# size of 16 or 32.\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "# We'll take training samples in random order. \r\n",
        "\r\n",
        "train_dataloader = DataLoader(\r\n",
        "            train_dataset,  # The training samples.\r\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
        "            batch_size = batch_size # Trains with this batch size.\r\n",
        "        )\r\n",
        "\r\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
        "validation_dataloader = DataLoader(\r\n",
        "            val_dataset, # The validation samples.\r\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
        "            batch_size = batch_size # Evaluate with this batch size.\r\n",
        "        )\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([1420, 512]) torch.Size([1420, 512]) torch.Size([1420, 17])\n1,278 training samples\n  142 validation samples\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632991359405
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HELPING METHODS \r\n",
        "# Function to calculate the accuracy of our predictions vs labels\r\n",
        "# TODO WRITE TUPLE OF LABEL AND PREDICTION TO CSV AS PREDICTION OUTPUT\r\n",
        "import csv\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "import pandas as pd\r\n",
        "import sys\r\n",
        "\r\n",
        "\r\n",
        "def write_predictions(preds, labels):\r\n",
        "\r\n",
        "    print(\"Diese Datei bitte umbenennen, sonst wird sie beim nÃ¤chsten Lauf Ã¼berschrieben.\")\r\n",
        "\r\n",
        "    with open('predictions.csv', 'w') as test_file:\r\n",
        "        file_writer = csv.writer(test_file)\r\n",
        "\r\n",
        "        for pred, label in zip(preds, labels):\r\n",
        "            file_writer.writerow((pred, label))\r\n",
        "    print(' File saved. ')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    # flatten multidimensional arrays to flat list\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    flat_acc = np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "    return flat_acc\r\n",
        "\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "outputs": [],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1633006157816
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "\r\n",
        "\r\n",
        "BERTmodel = BertForSequenceClassification.from_pretrained(\r\n",
        "    \"bert-base-german-dbmdz-cased\", # Use the 12-layer BERT model, with an uncased vocab.  bert-base-german-dbmdz-cased\r\n",
        "    num_labels = num_labels, # The number of output labels--2 for binary classification.\r\n",
        "                    # You can increase this for multi-class tasks.   \r\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\r\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "# lr = args.learning_rate - default is 5e-5, our notebook had 2e-5'\r\n",
        "# eps = args.adam_epsilon  - default is 1e-8.\r\n",
        "\r\n",
        "optimizer = AdamW(BERTmodel.parameters(),lr = 2e-5, eps = 1e-8 )\r\n",
        "\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "#  The BERT authors recommend between 2 and 4. \r\n",
        "\r\n",
        "epochs = 3\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "# (Note that this is not the same as the number of training samples).\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632991363747
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\r\n",
        "import tensorflow as tf\r\n",
        "import torch.nn.functional as F \r\n",
        "\r\n",
        "\r\n",
        "# Set the seed value all over the place to make this reproducible.\r\n",
        "seed_val = 42\r\n",
        "\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "# alternativly cuda for gpu\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "# store a number of quantities such as training and validation loss, \r\n",
        "# validation accuracy, and timings, raw predictions, the argmax of the prediction giving the index and hence the label and the true labels\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "raw_predictions, predictions, true_labels = [], [], []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "# For each epoch...\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode. Don't be mislead--the call to \r\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "    BERTmodel.train()\r\n",
        "\r\n",
        "    # For each batch of training data...\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 200 batches.\r\n",
        "        if step % 200 == 0 and not step == 0:\r\n",
        "            # Calculate elapsed time in minutes.\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            # Report progress.\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "        # `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)#.argmax(axis=1)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device).argmax(axis=1)\r\n",
        "\r\n",
        "\r\n",
        "        #print(b_input_ids.shape, b_input_mask.shape, b_labels.shape)\r\n",
        "        \r\n",
        "\r\n",
        "        # Always clear any previously calculated gradients before performing a\r\n",
        "        # backward pass. PyTorch doesn't do this automatically because \r\n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "        BERTmodel.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "        # The documentation for this `model` function is here: \r\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "        # It returns different numbers of parameters depending on what arguments\r\n",
        "        # arge given and what flags are set. For our useage here, it returns\r\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\r\n",
        "        # outputs prior to activation.\r\n",
        "        model_outputs = BERTmodel(b_input_ids, \r\n",
        "                             token_type_ids=None, \r\n",
        "                             attention_mask=b_input_mask, \r\n",
        "                             labels=b_labels)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        loss = model_outputs.loss\r\n",
        "        #logits == probability distribution before normalization using softmax or sigmoid.\r\n",
        "        logits = model_outputs.logits.detach().to(device)\r\n",
        "\r\n",
        "\r\n",
        "        prediction = np.argmax(logits[0]) #tf.nn.sigmoid(logits) \r\n",
        "        label_ids = b_labels.numpy()\r\n",
        "\r\n",
        "        raw_predictions.append(logits)\r\n",
        "        predictions.append(prediction)\r\n",
        "        true_labels.append(b_input_ids)\r\n",
        "        \r\n",
        "   \r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        #print('loss. ', loss)\r\n",
        "        total_train_loss += loss.item()\r\n",
        "\r\n",
        "\r\n",
        "        # Perform a backward pass to calculate the gradients.\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "        torch.nn.utils.clip_grad_norm_(BERTmodel.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters and take a step using the computed gradient.\r\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "        # modified based on their gradients, the learning rate, etc.\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    # Measure how long this epoch took.\r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "    # After the completion of each training epoch, measure our performance on\r\n",
        "    # our validation set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "    # during evaluation.\r\n",
        "    BERTmodel.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    eval_raw_predictions, eval_predictions, eval_true_labels = [], [], []\r\n",
        "\r\n",
        "\r\n",
        "    # Evaluate data for one epoch\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \r\n",
        "        # the `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)#.argmax(axis=1)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device).argmax(axis=1)\r\n",
        "        \r\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\r\n",
        "        # the forward pass, since this is only needed for backprop (training).\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            # Forward pass, calculate logit predictions.\r\n",
        "            # token_type_ids is the same as the \"segment ids\", which \r\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "            # The documentation for this `model` function is here: \r\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "            # values prior to applying an activation function like the softmax or sigmoid.\r\n",
        "            model_val_outputs = BERTmodel(b_input_ids, \r\n",
        "                                   token_type_ids=None, \r\n",
        "                                   attention_mask=b_input_mask,\r\n",
        "                                   labels=b_labels)\r\n",
        "\r\n",
        "        #which loss to use? FINE TUNING POTENTIAL\r\n",
        "        #labels = torch.tensor([1,0]).unsqueeze(0)\r\n",
        "        #outputs = BERTmodel(input_ids, attention_mask=b_input_mask)\r\n",
        "        #loss = F.cross_entropy(labels, outputs[0])\r\n",
        "\r\n",
        "\r\n",
        "        loss = model_val_outputs.loss\r\n",
        "        logits = model_val_outputs.logits.detach().to(device)\r\n",
        "        eval_prediction = np.argmax(logits[0]) #tf.nn.softmax(logits)\r\n",
        "\r\n",
        "\r\n",
        "        eval_raw_predictions.append(logits)\r\n",
        "        eval_predictions.append(eval_prediction)\r\n",
        "        eval_true_labels.append(b_input_ids)\r\n",
        "        \r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += loss.item()\r\n",
        "        #total_eval_loss += loss\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = logits.detach().to(device).numpy()\r\n",
        "        label_ids = b_labels.to(device).numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy for this batch of test sentences, and\r\n",
        "        # accumulate it over all batches.\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final accuracy for this validation run.\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    \r\n",
        "    # Measure how long the validation run took.\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "    \r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Record all statistics from this epoch.\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")\r\n",
        "\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n======== Epoch 1 / 3 ========\nTraining...\n\n  Average training loss: 2.01\n  Training epcoh took: 0:12:12\n\nRunning Validation...\n  Accuracy: 0.26\n  Validation Loss: 2.22\n  Validation took: 0:00:15\n\n======== Epoch 2 / 3 ========\nTraining...\n\n  Average training loss: 2.14\n  Training epcoh took: 0:08:45\n\nRunning Validation...\n  Accuracy: 0.28\n  Validation Loss: 2.18\n  Validation took: 0:00:16\n\n======== Epoch 3 / 3 ========\nTraining...\n\n  Average training loss: 2.11\n  Training epcoh took: 0:08:56\n\nRunning Validation...\n  Accuracy: 0.28\n  Validation Loss: 2.18\n  Validation took: 0:00:17\n\nTraining complete!\nTotal training took 0:30:43 (h:mm:ss)\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1633005147491
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fails with ValueError: Length of values (240) does not match length of index (10556)\r\n",
        "#write_predictions_to_origin('one_label_one_hot_nkm_open.csv', predictions, labels)\r\n",
        "\r\n",
        "write_predictions(predictions, labels)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Diese Datei bitte umbenennen, sonst wird sie beim nÃ¤chsten Lauf Ã¼berschrieben.\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1633006900921
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "torch.save({\r\n",
        "        'model_state_dict': BERTmodel.state_dict(),\r\n",
        "        'optimizer_state_dict': optimizer.state_dict()}, 'one_hot_nkm_bert.pth')\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632849799778
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/javaidnabi/toxic-comment-classification-using-bert/\r\n",
        "\r\n",
        "https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b\r\n",
        "\r\n",
        "https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}